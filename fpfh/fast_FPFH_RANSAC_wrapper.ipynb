{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "import open3d as o3d\n",
    "import cv2\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "from utilities import PointCloudGen, get_angular_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FPFH_RANSAC:\n",
    "\n",
    "    @staticmethod\n",
    "    def preprocess_point_cloud(pcd: PointCloudGen, voxel_size):\n",
    "        pcd_down = copy.deepcopy(pcd)\n",
    "\n",
    "        radius_normal = voxel_size * 2\n",
    "        pcd_down.estimate_normals(\n",
    "            o3d.geometry.KDTreeSearchParamHybrid(radius=radius_normal, max_nn=30))\n",
    "\n",
    "        radius_feature = voxel_size * 5\n",
    "        pcd_fpfh = o3d.pipelines.registration.compute_fpfh_feature(\n",
    "            pcd_down,\n",
    "            o3d.geometry.KDTreeSearchParamHybrid(radius=radius_feature, max_nn=100))\n",
    "        return pcd_down, pcd_fpfh\n",
    "\n",
    "    @staticmethod        \n",
    "    def run(cur_pc: PointCloudGen, voxel_size = 0.05, verbose = True,\n",
    "            transform_inplace = True):\n",
    "        start = time.time()\n",
    "\n",
    "        source_down, source_fpfh = FPFH_RANSAC.preprocess_point_cloud(cur_pc.pcd_visible, voxel_size)\n",
    "        target_down, target_fpfh = FPFH_RANSAC.preprocess_point_cloud(cur_pc.pcd_original, voxel_size)\n",
    "\n",
    "        distance_threshold = voxel_size * 2\n",
    "\n",
    "        result = o3d.pipelines.registration.registration_ransac_based_on_feature_matching(\n",
    "            source_down, target_down, source_fpfh, target_fpfh, True, distance_threshold,\n",
    "            o3d.pipelines.registration.TransformationEstimationPointToPoint(False),\n",
    "            4, [\n",
    "                o3d.pipelines.registration.CorrespondenceCheckerBasedOnEdgeLength(\n",
    "                    0.9),\n",
    "                o3d.pipelines.registration.CorrespondenceCheckerBasedOnDistance(\n",
    "                    distance_threshold)\n",
    "            ], o3d.pipelines.registration.RANSACConvergenceCriteria(1000, 200))\n",
    "\n",
    "        end = time.time()\n",
    "        \n",
    "        if transform_inplace:\n",
    "            cur_pc.pcd_visible.transform(result.transformation)\n",
    "\n",
    "        expected_transformation = cur_pc.expected_transformation()\n",
    "        estimated_transformation = result.transformation\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"Expected rotation: \")\n",
    "            print(expected_transformation[:3, :3])\n",
    "            print(\"Estimated rotation: \")\n",
    "            print(estimated_transformation[:3, :3])\n",
    "            print(\"Error (rad): \")\n",
    "            print(get_angular_error(expected_transformation[:3,:3], estimated_transformation[:3, :3]))\n",
    "\n",
    "            print(\"Expected translation: \")\n",
    "            print(expected_transformation[:3, 3])\n",
    "            print(\"Estimated translation: \")\n",
    "            print(estimated_transformation[:3, 3])\n",
    "            print(\"Error (m): \")\n",
    "            print(np.linalg.norm(expected_transformation[:3, 3] - estimated_transformation[:3, 3]))\n",
    "\n",
    "            print(\"Time taken (s): \", end - start)\n",
    "\n",
    "        return {\n",
    "            \"expected_rotation\": expected_transformation[:3, :3],\n",
    "            \"estimated_rotation\": estimated_transformation[:3, :3],\n",
    "            \"error_rotation\": get_angular_error(expected_transformation[:3,:3], estimated_transformation[:3, :3]),\n",
    "            \"expected_translation\": expected_transformation[:3, 3],\n",
    "            \"estimated_translation\": estimated_transformation[:3, 3],\n",
    "            \"error_translation\": np.linalg.norm(expected_transformation[:3, 3] - estimated_transformation[:3, 3]),\n",
    "            \"time_taken\": end - start\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "STORE_TEXTUAL_RESULTS = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ply_path = \"./data/office_chair_ext_points.ply\"\n",
    "cur_pc = PointCloudGen(ply_path)\n",
    "\n",
    "voxel_sizes = [\n",
    "    0.03, 0.02, 0.05\n",
    "]\n",
    "\n",
    "voxel_size_dirs = {\n",
    "    0.03: \"./outputs/office_chair/3K_points/\",\n",
    "    0.02: \"./outputs/office_chair/6K_points/\",\n",
    "    0.05: \"./outputs/office_chair/1K_points/\"\n",
    "}\n",
    "\n",
    "test_rotations = [\n",
    "    [60, 10, 10],\n",
    "    [30, 5, 5,],\n",
    "    [10, 60, 10],\n",
    "    [5, 30, 5],\n",
    "    [10, 10, 60],\n",
    "    [5, 5, 30],\n",
    "    [45, 45, 10],\n",
    "    [45, 10, 45],\n",
    "    [10, 45, 45],\n",
    "    [120, 10, 10],\n",
    "    [10, 120, 10],\n",
    "    [10, 10, 120]\n",
    "]\n",
    "\n",
    "test_translations = [\n",
    "    [0, 0, 0],\n",
    "    [1, 0, 0],\n",
    "    [0, 1, 0],\n",
    "    [0, 0, 1],\n",
    "    [1, 1, 0],\n",
    "    [0, 1, 1],\n",
    "    [1, 0, 1],\n",
    "    [1, 1, 1],\n",
    "    [0.2, 0.2, 0],\n",
    "    [0.4, 0.1, 0.4],\n",
    "    [0.3, 0.7, 0.1]\n",
    "]\n",
    "\n",
    "d = cur_pc.pcd_original_diameter\n",
    "camera_positions = [\n",
    "    [d, d, 0],\n",
    "    [0, d, -d],\n",
    "    [d, 0, d],\n",
    "    [0, d, d]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating pcds: 100%|██████████| 1584/1584 [14:09<00:00,  2.48it/s]"
     ]
    }
   ],
   "source": [
    "# Takes 10 minutes on i7\n",
    "\n",
    "progress_bar = tqdm(total=len(test_rotations) * len(test_translations) * len(camera_positions) * len(voxel_sizes), desc='Creating pcds')\n",
    "\n",
    "for voxel_size in voxel_sizes:\n",
    "    cur_source_base_dir = voxel_size_dirs[voxel_size]\n",
    "\n",
    "    frame_counter = 0\n",
    "\n",
    "    for rot in test_rotations:\n",
    "        for trans in test_translations:\n",
    "            for cur_camera_pos in camera_positions:\n",
    "\n",
    "                source_dir = os.path.join(cur_source_base_dir, f\"{frame_counter}\")\n",
    "\n",
    "                cur_pc = PointCloudGen()\n",
    "                cur_pc.load_from_dir(source_dir)\n",
    "\n",
    "                results = FPFH_RANSAC.run(cur_pc, voxel_size = voxel_size, verbose=False)\n",
    "                \n",
    "                files_to_remove = [\n",
    "                    os.path.join(source_dir, \"03_fast_FPFH_RANSAC_output_transformation.png\"),\n",
    "                    os.path.join(source_dir, \"03_fast_FPFH_RANSAC_results.pkl\"),\n",
    "                    os.path.join(source_dir, \"03_fast_FPFH_RANSAC_results.txt\")\n",
    "                ]\n",
    "                \n",
    "                for file_path in files_to_remove:\n",
    "                    if os.path.exists(file_path):\n",
    "                        os.remove(file_path)\n",
    "\n",
    "                cur_pc.save_viewport_image(os.path.join(source_dir, \"03_fast_FPFH_RANSAC_output_transformation.png\"), highlight_pcd_visible_colour=[0, 0, 1])\n",
    "\n",
    "                # for faster loading\n",
    "                with open(os.path.join(source_dir, \"03_fast_FPFH_RANSAC_results.pkl\"), \"wb\") as pickle_file:\n",
    "                    pickle.dump(results, pickle_file)\n",
    "\n",
    "                if STORE_TEXTUAL_RESULTS:\n",
    "                    results_str = \"\\n\".join([f\"{key}: {value}\" for key, value in results.items()])\n",
    "                    with open(os.path.join(source_dir, \"03_fast_FPFH_RANSAC_results.txt\"), 'w') as text_file:\n",
    "                        text_file.write(results_str)\n",
    "\n",
    "                frame_counter += 1\n",
    "                progress_bar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mobile-robotics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
