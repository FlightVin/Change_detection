{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run through one sequence, use the sequence class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p /scratch/aneesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "sys.path.append(os.path.join(os.getcwd(), \"Grounded-Segment-Anything\"))\n",
    "sys.path.append(os.path.join(os.getcwd(), \"Grounded-Segment-Anything\", \"GroundingDINO\"))\n",
    "sys.path.append(os.path.join(os.getcwd(), \"GroundingDINO\"))\n",
    "sys.path.append(os.path.join(os.getcwd(), \"recognize-anything\"))\n",
    "\n",
    "print(os.getcwd(), os.path.join(os.getcwd(), \"Grounded-Segment-Anything\", \"GroundingDINO\"))\n",
    "print(sys.path)\n",
    "\n",
    "import argparse\n",
    "import copy\n",
    "\n",
    "from IPython.display import display\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from torchvision.ops import box_convert\n",
    "\n",
    "# recognise anything\n",
    "from ram.models import ram\n",
    "from ram import inference_ram as inference\n",
    "from ram import get_transform\n",
    "\n",
    "# Grounding DINO\n",
    "import GroundingDINO.groundingdino.datasets.transforms as T\n",
    "from GroundingDINO.groundingdino.models import build_model\n",
    "from GroundingDINO.groundingdino.util import box_ops\n",
    "from GroundingDINO.groundingdino.util.slconfig import SLConfig\n",
    "from GroundingDINO.groundingdino.util.utils import clean_state_dict, get_phrases_from_posmap\n",
    "from GroundingDINO.groundingdino.util.inference import annotate, load_image, predict\n",
    "\n",
    "import supervision as sv\n",
    "\n",
    "# segment anything\n",
    "from segment_anything import build_sam, SamPredictor\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# diffusers\n",
    "import PIL\n",
    "import requests\n",
    "import torch\n",
    "from io import BytesIO\n",
    "from diffusers import StableDiffusionInpaintPipeline\n",
    "\n",
    "\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load models\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def load_model_hf(repo_id, filename, ckpt_config_filename, device='cpu'):\n",
    "    cache_config_file = hf_hub_download(repo_id=repo_id, filename=ckpt_config_filename)\n",
    "\n",
    "    args = SLConfig.fromfile(cache_config_file)\n",
    "    args.device = device\n",
    "    model = build_model(args)\n",
    "\n",
    "    cache_file = hf_hub_download(repo_id=repo_id, filename=filename)\n",
    "    checkpoint = torch.load(cache_file, map_location=device)\n",
    "    log = model.load_state_dict(clean_state_dict(checkpoint['model']), strict=False)\n",
    "    print(\"Model loaded from {} \\n => {}\".format(cache_file, log))\n",
    "    _ = model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_repo_id = \"ShilongLiu/GroundingDINO\"\n",
    "ckpt_filenmae = \"groundingdino_swinb_cogcoor.pth\"\n",
    "ckpt_config_filename = \"GroundingDINO_SwinB.cfg.py\"\n",
    "\n",
    "\n",
    "groundingdino_model = load_model_hf(ckpt_repo_id, ckpt_filenmae, ckpt_config_filename, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# if not os.path.isfile(\"/scratch/aneesh/ram_swin_large_14m.pth\"):\n",
    "# !wget -O /scratch/aneesh/ram_swin_large_14m.pth https://huggingface.co/spaces/xinyu1205/recognize-anything/resolve/main/ram_swin_large_14m.pth\n",
    "\n",
    "# r = ram(image_size=384, vit='swin_l')\n",
    "ram_model = ram(pretrained='/scratch/aneesh/ram_swin_large_14m.pth', image_size=384, vit='swin_l')\n",
    "# ram_model.eval()\n",
    "# ram_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "test_img_path = '/home2/aneesh.chavan/Change_detection/360_zip/view2/view2.png'\n",
    "transform = get_transform(image_size=384)\n",
    "image = transform(Image.open(test_img_path)).unsqueeze(0).to(device)\n",
    "\n",
    "res = inference(image, ram_model)\n",
    "print(\"Image Tags: \", res[0])\n",
    "\n",
    "plt.imshow(Image.open(test_img_path));\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! wget -O /scratch/aneesh/sam_vit_h_4b8939.pth https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\n",
    "\n",
    "sam_checkpoint = '/scratch/aneesh/sam_vit_h_4b8939.pth'\n",
    "\n",
    "sam_predictor = SamPredictor(build_sam(checkpoint=sam_checkpoint).to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grounding DINO for detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detect object using grounding DINO\n",
    "def detect(image, text_prompt, model, image_source=None, box_threshold = 0.35, text_threshold = 0.55, remove_combined=False):\n",
    "  boxes, logits, phrases = predict(\n",
    "      model=model,\n",
    "      image=image,\n",
    "      caption=text_prompt,\n",
    "      box_threshold=box_threshold,\n",
    "      text_threshold=text_threshold\n",
    "  )\n",
    "\n",
    "  if type(image_source) == None:\n",
    "    annotated_frame = None\n",
    "  else:\n",
    "    annotated_frame = annotate(image_source=image_source, boxes=boxes, logits=logits, phrases=phrases)\n",
    "    annotated_frame = annotated_frame[...,::-1] # BGR to RGB\n",
    "\n",
    "  return annotated_frame, boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download_image(image_url, local_image_path)\n",
    "test_img_path = '/home2/aneesh.chavan/Change_detection/360_zip/view2/view2.png'\n",
    "image_source, image = load_image(test_img_path)\n",
    "Image.fromarray(image_source)\n",
    "\n",
    "annotated_frame, detected_boxes = detect(image, text_prompt=\"sofa . chair . table\",\n",
    "                                         model=groundingdino_model,\n",
    "                                         image_source=image_source)\n",
    "Image.fromarray(annotated_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# object retrieval functions\n",
    "\n",
    "# return a tensor containing all bounding boxes\n",
    "# filter for duplicates by calculating\n",
    "\n",
    "# [TODO] speedup\n",
    "\n",
    "import time\n",
    "\n",
    "def getIoU(rect1, rect2):\n",
    "  area_rect1 = rect1[2]*rect1[3]\n",
    "  area_rect2 = rect2[2]*rect2[3]\n",
    "\n",
    "  overlap_top_left = (max(rect1[0], rect2[0]), max(rect1[1], rect2[1]))\n",
    "  overlap_bottom_right = (min(rect1[0] + rect1[2], rect2[0] + rect2[2]), min(rect1[1] + rect1[3], rect2[1] + rect2[3]))\n",
    "\n",
    "  if (overlap_bottom_right[0] <= overlap_top_left[0]) or (overlap_bottom_right[1] <= overlap_top_left[1]):\n",
    "    return 0.0  # No overlap, return 0% overlap\n",
    "\n",
    "  # Calculate the area of the overlap rectangle\n",
    "  overlap_area = abs((overlap_bottom_right[0] - overlap_top_left[0]) * (overlap_bottom_right[1] - overlap_top_left[1]))\n",
    "  percent_overlap = (overlap_area / min(area_rect1, area_rect2))\n",
    "\n",
    "  return percent_overlap\n",
    "\n",
    "def compSize(rect1, rect2):\n",
    "  area_rect1 = rect1[2]*rect1[3]\n",
    "  area_rect2 = rect2[2]*rect2[3]\n",
    "\n",
    "  diff = min(area_rect1, area_rect2)/max(area_rect1, area_rect2)\n",
    "  return diff\n",
    "\n",
    "def getAllDetectedBoxes(image, image_source=None, keywords=[], show=False, intersection_threshold=0.7, size_threshold=0.75):\n",
    "  total_time = 0\n",
    "\n",
    "  with torch.no_grad():\n",
    "    boxes = []\n",
    "    unique_boxes_num = 0\n",
    "\n",
    "    for i, word in enumerate(keywords):\n",
    "      af, detected = detect(image, image_source=image_source, text_prompt=str(word), model=groundingdino_model)\n",
    "\n",
    "      cnt_time = time.time()\n",
    "\n",
    "      # # limit edges\n",
    "      # for d in detected:\n",
    "      #   if d[0] + d[2] >= 1:\n",
    "      #     d[2] = 1 - d[0]\n",
    "\n",
    "      #   if d[1] + d[3] >= 1:\n",
    "      #     d[3] = 1 - d[1]\n",
    "\n",
    "      if show:\n",
    "        print(i)\n",
    "      unique_enough = True\n",
    "\n",
    "      if detected != None and len(detected) != 0:\n",
    "        if unique_boxes_num == 0:\n",
    "          for box in detected:\n",
    "            boxes.append(box)\n",
    "            unique_boxes_num += 1\n",
    "\n",
    "          if show and type(image_source) != None:\n",
    "            Image.fromarray(af).show()\n",
    "\n",
    "            if show:\n",
    "              print(\"detected\", detected)\n",
    "\n",
    "        else:\n",
    "          print(\"boxes: \", boxes)\n",
    "          for box in detected:\n",
    "            unique_enough = True\n",
    "\n",
    "            if show:\n",
    "              print(\"detected: \", detected)\n",
    "\n",
    "            for prev in boxes[:unique_boxes_num]:\n",
    "\n",
    "              iou = getIoU(box, prev)\n",
    "              diff = compSize(box, prev)\n",
    "\n",
    "              if show:\n",
    "                print(\"comparing; -- \", prev, box)\n",
    "                print(\"iou: \", iou)\n",
    "                print(\"diff: \", diff)\n",
    "\n",
    "              if (iou > intersection_threshold and diff > size_threshold):\n",
    "                # bounding box is not unique enough to be added\n",
    "                unique_enough = False\n",
    "\n",
    "                if show:\n",
    "                  print(\"failed\")\n",
    "                break\n",
    "\n",
    "            if unique_enough:\n",
    "              boxes.append(box)\n",
    "              unique_boxes_num += 1\n",
    "\n",
    "              if show:\n",
    "                print(\"         success!\")\n",
    "                print(boxes)\n",
    "\n",
    "          if show and type(image_source) != None:\n",
    "            plt.imshow(af)\n",
    "\n",
    "      total_time += (time.time() - cnt_time)\n",
    "\n",
    "    # print(total_time)\n",
    "    return torch.stack(boxes)\n",
    "  \n",
    "def decide_uniqueness(candidate_boxes, stored_boxes, intersection_threshold=0.7, size_threshold=0.75):\n",
    "  # get area difference\n",
    "  candidate_areas = 4 * candidate_boxes[:,2] * candidate_boxes[:,3]\n",
    "  stored_areas = 4 * stored_boxes[:,2] * stored_boxes[:,3]\n",
    "  minimum_areas = np.minimum(candidate_areas.unsqueeze(1), stored_areas)\n",
    "\n",
    "  area_diff = candidate_areas.unsqueeze(1)/stored_areas\n",
    "  area_diff[area_diff >= 1.] = 1/area_diff[area_diff >= 1.]\n",
    "\n",
    "  conv_cb = candidate_boxes.clone()\n",
    "  conv_sb = stored_boxes.clone()\n",
    "\n",
    "  conv_cb[:, :2] -= conv_cb[:, 2:]\n",
    "  conv_cb[:, 2:] = 2 * conv_cb[:, 2:] + conv_cb[:, :2]\n",
    "  conv_cb = np.expand_dims(conv_cb, axis=1)\n",
    "\n",
    "  conv_sb[:, :2] -= conv_sb[:, 2:]\n",
    "  conv_sb[:, 2:] = 2 * conv_sb[:, 2:] + conv_sb[:, :2]\n",
    "\n",
    "  overlap_boxes = np.concatenate([np.maximum(conv_cb[...,:2], conv_sb[...,:2]),\n",
    "                                  np.minimum(conv_cb[...,2:], conv_sb[...,2:])],\n",
    "                                 axis=-1)\n",
    "\n",
    "  iou = np.where(np.logical_and((overlap_boxes[..., 2] > overlap_boxes[..., 0]), (overlap_boxes[..., 3] > overlap_boxes[..., 1])),\n",
    "                 (overlap_boxes[..., 3] - overlap_boxes[..., 1]) * (overlap_boxes[..., 2] - overlap_boxes[..., 0])/minimum_areas,\n",
    "                 -np.inf)\n",
    "\n",
    "  boxes_comparison = np.where(\n",
    "      np.logical_and(np.logical_and(iou > intersection_threshold, area_diff > size_threshold), iou != -np.inf),\n",
    "      False,\n",
    "      True\n",
    "  )\n",
    "\n",
    "  unique_enough = np.logical_and.reduce(boxes_comparison, 1)\n",
    "\n",
    "  return unique_enough\n",
    "\n",
    "def eff_getAllDetectedBoxes(image, image_source=None, keywords=[], show=False, intersection_threshold=0.7, size_threshold=0.75):\n",
    "  with torch.no_grad():\n",
    "    boxes = None\n",
    "    unique_boxes_num = 0\n",
    "\n",
    "    total_time = 0\n",
    "\n",
    "    for i, word in enumerate(keywords):\n",
    "      af, detected = detect(image, image_source=image_source, text_prompt=str(word), model=groundingdino_model)\n",
    "\n",
    "      cnt_time = time.time()\n",
    "\n",
    "      if show:\n",
    "        print(i)\n",
    "      # unique_enough = True\n",
    "\n",
    "      # sort through all detected boxes, add them if there is little enough overlap with all recorded bboxes, or it is small enough for overlap to not matter\n",
    "      if detected != None and len(detected) != 0:\n",
    "        if boxes == None:\n",
    "          boxes = detected\n",
    "\n",
    "          if show and type(image_source) != None:\n",
    "            Image.fromarray(af).show()\n",
    "\n",
    "            if show:\n",
    "              print(\"detected\", detected)\n",
    "\n",
    "        else:\n",
    "          if show:\n",
    "            print(\"boxes:\\n\", boxes)\n",
    "\n",
    "            if type(image_source) != None:\n",
    "              Image.fromarray(af).show()\n",
    "\n",
    "\n",
    "          unique_enough = decide_uniqueness(detected, boxes)\n",
    "          boxes = torch.concat([boxes] + [detected[num].unsqueeze(0) for num, val in enumerate(unique_enough) if val])\n",
    "\n",
    "          if show:\n",
    "            for i, k in enumerate(unique_enough):\n",
    "              print(\"Added \" if k else \"Failed \", sep='')\n",
    "              print(detected[i])\n",
    "\n",
    "\n",
    "          total_time += (time.time() - cnt_time)\n",
    "\n",
    "    print(total_time)\n",
    "    return boxes\n",
    "  \n",
    "# segmentation code\n",
    "\n",
    "# THERE IS SPACE TO BATCH SEGMENTATIONS\n",
    "\n",
    "def segment(image, sam_model, boxes):\n",
    "  sam_model.set_image(image)\n",
    "  H, W, _ = image.shape\n",
    "  boxes_xyxy = box_ops.box_cxcywh_to_xyxy(boxes) * torch.Tensor([W, H, W, H])\n",
    "\n",
    "  transformed_boxes = sam_model.transform.apply_boxes_torch(boxes_xyxy.to(device), image.shape[:2])\n",
    "  masks, _, _ = sam_model.predict_torch(\n",
    "      point_coords = None,\n",
    "      point_labels = None,\n",
    "      boxes = transformed_boxes,\n",
    "      multimask_output = False,\n",
    "      )\n",
    "  return boxes_xyxy, masks.cpu()\n",
    "\n",
    "\n",
    "def draw_mask(mask, image, random_color=True):\n",
    "    if random_color:\n",
    "        color = np.concatenate([np.random.random(3), np.array([0.8])], axis=0)\n",
    "    else:\n",
    "        color = np.array([30/255, 144/255, 255/255, 0.6])\n",
    "    h, w = mask.shape[-2:]\n",
    "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "\n",
    "    annotated_frame_pil = Image.fromarray(image).convert(\"RGBA\")\n",
    "    mask_image_pil = Image.fromarray((mask_image.cpu().numpy() * 255).astype(np.uint8)).convert(\"RGBA\")\n",
    "\n",
    "    return np.array(Image.alpha_composite(annotated_frame_pil, mask_image_pil))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detected_boxes = getAllDetectedBoxes(image, image_source, [l for l in \"sofa | chair | table\".split('|')],\n",
    "                                     show=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bboxs, masks = segment(image_source, sam_predictor, boxes=detected_boxes)\n",
    "\n",
    "for idx in range(masks.shape[0]):\n",
    "  annotated_frame_with_mask = draw_mask(masks[idx][0], annotated_frame)\n",
    "  plt.figure()\n",
    "  plt.imshow(annotated_frame_with_mask)\n",
    "#   Image.fromarray(annotated_frame_with_mask).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bboxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mt = np.copy(image_source)\n",
    "mt[masks[0,0] == False] = 0\n",
    "plt.imshow(mt[290:385, 370:500,  :])\n",
    "plt.figure()\n",
    "plt.imshow(mt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth_img = np.load(\"360_zip/view2/view2.npy\")\n",
    "\n",
    "plt.imshow(depth_img, cmap='gray')\n",
    "plt.colorbar()  # Adds a colorbar to show the depth values\n",
    "plt.title(\"Depth Image\")\n",
    "plt.show()\n",
    "\n",
    "# print(depth_img, np.max(depth_img), np.min(depth_img))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth_test = np.copy(depth_img)\n",
    "depth_test[(masks[0] == False).squeeze()] = 0\n",
    "plt.imshow(depth_test, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1,2,3]\n",
    "b = [1,2,3]\n",
    "c = [1,2,3]\n",
    "d = [1,2,3]\n",
    "e = [1,2,3]\n",
    "\n",
    "x = [[1,1,1,1,1], [1,1,1,1,1], [1,1,1,1,1], [1,1,1,1,1], [1,1,1,1,1]]\n",
    "y = [[2,2,2,2,2], [2,2,2,2,2], [2,2,2,2,2], [2,2,2,2,2], [2,2,2,2,2]]\n",
    "z = [[4,4,4,4,4], [4,4,4,4,4], [4,4,4,4,4], [4,4,4,4,4], [4,4,4,4,4]]\n",
    "\n",
    "np.stack([x,y,z]).reshape(3, -1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(360000, 3)\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "f = 300\n",
    "\n",
    "w, h = depth_test.shape\n",
    "\n",
    "row_wise = np.tile(np.linspace(-h/2, h/2, h, dtype=np.float32), (w, 1))\n",
    "col_wise = np.tile(np.linspace(w/2, -w/2, w, dtype=np.float32).reshape(1,-1).T, (1, h))\n",
    "\n",
    "X = row_wise * depth_img/f\n",
    "Y = col_wise * depth_img/f\n",
    "Z = depth_img\n",
    "\n",
    "# zeroth object centroid\n",
    "centroid0 = np.array([\n",
    "    np.where(masks[0] == True, X, 0).sum(),\n",
    "    np.where(masks[0] == True, Y, 0).sum(),\n",
    "    np.where(masks[0] == True, depth_img, 0).sum()\n",
    "]) /np.where(masks[0] == True, 1, 0).sum()\n",
    "\n",
    "centroid1 = np.array([\n",
    "    np.where(masks[1] == True, X, 0).sum(),\n",
    "    np.where(masks[1] == True, Y, 0).sum(),\n",
    "    np.where(masks[1] == True, depth_img, 0).sum()\n",
    "]) /np.where(masks[1] == True, 1, 0).sum()\n",
    "\n",
    "centroid2 = np.array([\n",
    "    np.where(masks[2] == True, X, 0).sum(),\n",
    "    np.where(masks[2] == True, Y, 0).sum(),\n",
    "    np.where(masks[2] == True, depth_img, 0).sum()\n",
    "]) /np.where(masks[2] == True, 1, 0).sum()\n",
    "\n",
    "# for i in range(w):\n",
    "#   for h in range(h):\n",
    "#     # X = z*x/f\n",
    "\n",
    "X1 = np.where(masks[0] == True, X, 0).reshape(600,600)\n",
    "Y1 = np.where(masks[0] == True, Y, 0).reshape(600,600)\n",
    "Z1 = np.where(masks[0] == True, depth_img, 0).reshape(600,600)\n",
    "\n",
    "pcd1 = np.stack([X1, Y1, Z1]).reshape(3, -1).T\n",
    "# pcd1 = np.stack([X, Y, Z]).reshape(3, -1).T\n",
    "print(pcd1.shape)\n",
    "\n",
    "print(pcd1)\n",
    "\n",
    "# pcd1 = pcd1[np.logical_and(pcd1[:,0] != 0. , pcd1[:,1] != 0. , pcd1[:,2] != 0.)]\n",
    "# print(pcd1.shape)\n",
    "\n",
    "# plt.figure()\n",
    "# plt.title('X')\n",
    "# plt.imshow(X1[:,200:])\n",
    "# plt.colorbar()\n",
    "# plt.show()\n",
    "\n",
    "# plt.figure()\n",
    "# plt.title('Y')\n",
    "# plt.imshow(Y1[:,200:])\n",
    "# plt.colorbar()\n",
    "# plt.show()\n",
    "\n",
    "# plt.figure()\n",
    "# plt.title('Z')\n",
    "# plt.imshow(Z1[:,200:])\n",
    "# plt.colorbar()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import open3d as o3d\n",
    "import plotly.graph_objects as go\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcd1_o3d = o3d.geometry.PointCloud(o3d.utility.Vector3dVector(pcd1))\n",
    "\n",
    "\n",
    "fig = go.Figure(\n",
    "    data=[\n",
    "        go.Scatter3d(\n",
    "            x=pcd1[:,0], y=pcd1[:,1], z=pcd1[:,2], \n",
    "            mode='markers',\n",
    "            marker=dict(size=1)\n",
    "        )\n",
    "    ],\n",
    "    layout=dict(\n",
    "        scene=dict(\n",
    "            xaxis=dict(visible=False),\n",
    "            yaxis=dict(visible=False),\n",
    "            zaxis=dict(visible=False)\n",
    "        )\n",
    "    )\n",
    ")\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reid",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
